<!doctype html>
<html lang="en">

<head>
    <meta charset="utf-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">

    <title>5B 2</title>

    <link rel="stylesheet" href="style.css">
    <link href="https://fonts.googleapis.com/css2?family=Montserrat:wght@300;400;600&family=Merriweather:wght@300;400;700&display=swap" rel="stylesheet">

    <script defer src="https://use.fontawesome.com/releases/v5.7.2/js/all.js"
        integrity="sha384-0pzryjIRos8mFBWMzSSZApWtPl/5++eIfzYmTgBBmXYdhvxPc+XcFEk+zJwDgWbP"
        crossorigin="anonymous"></script>
    <script id="MathJax-script" async src="https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-mml-chtml.js"></script>
</head>

<body>
    <header>
        <h1>Part 5B 2</h1>
    </header>

    <div class="container">

        <!-- Contents -->
        <main>

            <section id="Results">
                <h2> Time Conditioning </h2>

                <h3> Overview </h3>
                The algorithm used is exactly the same one as before for iterative denoising: except we now determine the alphas and betas ourselves
                and don't have a trained model to start with. At a high level, 
                we choose a random image from the training set, random, and train the denoiser to 
                predict noise in \( x_t \). This ensures that we are able to train the model on both a diverse 
                dataset of images and timesteps parameter \( t\). We repeat this until the model converges and we are happy. 
                <figure class="gallery-item">
                    <img src="../B_results/ims/tc_train.png" alt="Image 1"
                    style="width: 768px; height: 256px;"
                 \>
                 </figure>
                 <br>
                 In order to apply time-conditioning into the UNet, we use the following architecture: 
                 <figure class="gallery-item">
                    <img src="../B_results/ims/tc_arc.png" alt="Image 1"
                    style="width: 768px; height: 512px;"
                 \>
                 </figure>
                 <br>
                 with the following new block for introducing parameter \( t\):
                 <figure class="gallery-item">
                    <img src="../B_results/ims/new_op.png" alt="Image 1"
                    style="width: 512px; height: 128px;"
                 \>
                 </figure>
                 <br>
                Parameters used in both parts: <code> num_epochs = 20 </code>, <code>batch_size = 128</code>,
                <code> D = 64 </code>, <code>num_ts = 300</code>, and a threshold of 0.02 for Gradient Descent. 
                An Adam optimizer with initial <code>lr =1e-3 </code> is chosen, with exponential learning rate 
                decay with gamma of \( (0.1)^{1/\texttt{num_epochs}} \). For Classifier-Free Guidance, we use 
                \( \gamma = 5 \).
                <h3> Training loss curve </h3>

                <figure class="gallery-item">
                    <img src="../B_results/B2/training_loss_c.png" alt="Image 1"
                 \>
                 </figure>

                 <h3> Sampling after 5 epochs  </h3>
                 For sampling, we use the following algorithm: once again, other than the choice of model 
                 it is identical to the approach from part (a): we sample random noise and then denoise it 
                 to generate a random digit. 

                 <figure class="gallery-item">
                    <img src="../B_results/ims/tc_sample.png" alt="Image 1"
                    style="width: 768px; height: 256px;"
                 \>
           </figure>
        <br>
           Following are the results with this algorithm: notice that we have no control over which 
           digits are generated, and also only about half of them look like sensible digits. In the next part, 
           we will use class conditioning to improve results of sampling. Obviously, the results get better with more 
           and more epochs. 

                 <figure class="gallery-item">
                     <img src="../B_results/B2/epoch05_time.png" alt="Image 1"
                     style="width: 560px; height: 224px;"
                  \>
            </figure>

            <h3> Sampling after 20 epochs  </h3>

            <figure class="gallery-item">
                <img src="../B_results/B2/epoch20_time.png" alt="Image 5"
                style="width: 560px; height: 224px;"
             \>
       </figure>

            <h3> Bells and Whistles: Sampling Digits GIF for UNet with Time Conditioning</h3>

            <div class="gallery">
                <img src="../B_results/B2/epoch05t_animation.gif" />
                <figcaption><code>epoch=5</code></figcaption>
                </div> 
                <br>
            <div class="gallery">
                    <img src="../B_results/B2/epoch20t_animation.gif" />
                    <figcaption><code>epoch=20</code></figcaption>
                    </div> 
    </section>


            <section id="Results">
                <h2> Class Conditioning </h2>

                <h3> Overview </h3>

                The idea here is similar to time conditioning, except we have added conditioning on class and use 
                Classifier-Free Guidance exactly as in part A. This leads to the following training algorithm: 
                <figure class="gallery-item">
                    <img src="../B_results/ims/cc_train.png" alt="Image 1"
                    style="width: 768px; height: 256px;"
                 \>
                 <br>
                 The model architecture is exactly the same as before, but instead of \(x_{i+1} \gets x_i + \text{t_block}\)
                 in the model we perform \(x_{i+1} \gets \text{c_block}*x_i + \text{t_block}\) to incorporate time and class.
                 Additionally, we have a dropout condition for uncondtional generation: we drop the one-hot encoded vector \( \mathbf{c} \)
                 to zero vector with probability \( p_{\text{uncond}} = 0.1 \). 
           </figure>
                <h3> Training loss curve </h3>

                <figure class="gallery-item">
                    <img src="../B_results/B2/training_loss_c.png" alt="Image 1"
                 \>
                 </figure>
                 <br>
                 Interestingly enough, notice that while training loss curves for time and class conditioning are very similar, 
                 there is still a big difference between the quality of sampling: class conditioning is far better and much more directed. 
                 The idea is that we are not improving our model, but denoising with specific intent using CFG, which leads to better samplings. 
                 <h3> Sampling after 5 epochs, all digits 4 times  </h3>
                    Since we can now guide (via CFG) our noise to denoise towards a certain 
                    class (label) of digit, generating/sampling digits from noise becomes a lot easier. 
                    In fact, we can specify exactly how many instances of a certain digit we want, and the 
                    model can give us that. This is exactly what is implemented below: here is sampling after 5 and 
                    20 epochs. 
                 <figure class="gallery-item">
                     <img src="../B_results/B2/epoch05_c.png" alt="Image 1"
                     style="width: 560px; height: 224px;"
                  \>
            </figure>

            <h3> Sampling after 20 epochs  </h3>

            <figure class="gallery-item">
                <img src="../B_results/B2/epoch20_c.png" alt="Image 5"
                style="width: 560px; height: 224px;"
             \>
       </figure>

            <h3> Bells and Whistles: Sampling Digits GIF for UNet with Class Conditioning</h3>
            
            <div class="gallery">
                <img src="../B_results/B2/epoch1_class_anim.gif" />
                <figcaption><code>epoch=1</code></figcaption>
                </div> 
                
                <br> 
            <div class="gallery">
                    <img src="../B_results/B2/epoch5_class_anim.gif" />
                    <figcaption><code>epoch=5</code></figcaption>
                    </div> 
                    <br>
                    <div class="gallery">
                        <img src="../B_results/B2/epoch10_class_anim.gif" />
                        <figcaption><code>epoch=10</code></figcaption>
                        </div> 

                        <br>

                        <div class="gallery">
                            <img src="../B_results/B2/epoch20_class_anim.gif" />
                            <figcaption><code>epoch=20</code></figcaption>
                            </div> 

          
            </section>

        </main>

    </div>

</body>

</html>

