<!doctype html>
<html lang="en">

<head>
    <meta charset="utf-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">

    <title>Project 4</title>

    <link rel="stylesheet" href="style.css">
    <link href="https://fonts.googleapis.com/css2?family=Montserrat:wght@300;400;600&family=Merriweather:wght@300;400;700&display=swap" rel="stylesheet">

    <script defer src="https://use.fontawesome.com/releases/v5.7.2/js/all.js"
        integrity="sha384-0pzryjIRos8mFBWMzSSZApWtPl/5++eIfzYmTgBBmXYdhvxPc+XcFEk+zJwDgWbP"
        crossorigin="anonymous"></script>
    <script id="MathJax-script" async src="https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-mml-chtml.js"></script>
</head>

<body>
    <header>
        <h1>Project 5A: The Power of Diffusion Models</h1>
        <p>Kishan Jani</p>
    </header>

    <div class="container">

        <!-- Contents -->
        <main>
            </section>
            <section id="part 0">
                <h2>Part 5A 0: Setup</h2>
                <p>
                    We will use the <code> DeepFloydIF </code> diffusion model. We use a random seed of <code> seed = 10</code>, 
                    here and throughout the project. In this part, we simply test out the model, generating images 
                    for 3 text prompts with captions. We do this across different values of <code> num_inference_steps </code>.
                </p>
                <br>
                To see the images generated, click on the following link: 
                <br>
                <a href="./0/index.html">5A Part 0 Results</a>
            </section>

            <section id="part 1.1">
                <h2>Part 5A 1.1: Implementing the Forward Porcess</h2>
                <h3> Overview </h3>
                <p>
                    The forward process is defined by 
                    \[ x_t = \sqrt{\bar{\alpha}_t}x_0 + \sqrt{1 - \bar{\alpha}_t}\varepsilon  \]
                    where \( \varepsilon \sim N(0,1) \). Here \(x_0\) is the clean image, the noisy image 
                    generated is \( x_t \), so we really are making the noisy image by sampling from a Gaussian 
                    of mean \( \sqrt{\bar{\alpha}_t} x_0\) and variance \( 1- \bar{\alpha}_t \). 

                    We perform the process on a test image of the campanile shown below, which we will resize 
                    to \( 64 \times 64 \). 
                </p>
                <h3> Results </h3>
                <p> To see results for \( t\in [250,500,750] \), click the link below: </p>

                <a href="./1.1/index.html">5A Part 1.1 Results</a>
                  
            </section>


            <section id = "part 3">

                <h2>
                    Part 4a.3: Image Rectification (Contains Deliverables)
                </h2>
                <h3> Introduction</h3>
                    By setting the target keypoints to be a well-defined rectangle in the homography, given an image, we can select a feature that we know
                is rectangular to "rectify" it i.e, make it straight. 
                <br>
                Results in this section are highly sensitive to selection of points. Firstly, it is crucial that the points selected are accurate, 
                since rectification empirically proved to be highly sensitive to this. Furthermore, the ordering of points needs to be correct (otherwise 
                unintelligible results are produced since the image gets "flipped over itself"). 
                <br>
                Finally, to produce aesthetic images, the dimensions of the new rectified feature were chosen to be somewhat similar to those of the old 
                feature. 

                <h3> Results </h3>

                We present some examples of image rectification. Click the following link for results. 
                            <br>
            <a href="./4a.3/index.html">Part 4a.3 Results</a>
            <h2>
                Part 4a.4: Image Mosaicing (Contains Deliverables)
            </h2>
                <h3>Introduction </h3>
                Finally, we can use the developed machinery to create a mosaic using several images. The strategy is to choose 
                several corresponding keypoints (from prominent features) between the two images, and warp the first image into 
                the second so that keypoints map to keypoints (as best as possible). 
                <br>
                With this, we will have two images whose keypoints roughly match. In order to properly blend these images, we 
                use a distance transform on the alpha channel. This way, during the blending, we give more weight to the image 
                which is further from the background at a certain point. Specifically, 
                \[\text{Blended Image}[i,j] = \mathbb{1}_k\{ \text{dist}_1[i,j] > \text{dist}_2[i,j] \}\cdot \text{Image}_1[i,j] + \big( 1 -
                \mathbb{1}_k\{ \text{dist}_1[i,j] > \text{dist}_2[i,j] \} \big) \cdot \text{Image}_2[i,j].\]
                In order to ensure that the blending is smooth, we use a Gaussian blur on the indicator/mask 
                \[\mathbb{1}_k\{ \text{dist}_1[i,j] > \text{dist}_2[i,j]\} = \text{Gaussian}(\sigma_k = 2^k \sigma_0, \text{window} =
                \lfloor 6\sigma_k \rfloor +1) \star \mathbb{1}\{ \text{dist}_1[i,j] > \text{dist}_2[i,j]\},\]
                skipping the first few steps in the Gaussian stack until the mask is sufficiently blurred. This ensures that there are no 
                edge artifacts in the final product. 
                
                <h3>Results </h3>
                Click the following link for results. Across all cases, we start with \(\sigma_0 = 7\) and skip the first \(2\)
                images from the Gaussian Stack of the mask, with depth \(5\). 
            <a href="./4a.4/index.html">Part 4a.4 Results</a>
              
            </section>
        </main>
    </div>

</body>

</html>
