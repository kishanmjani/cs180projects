<!doctype html>
<html lang="en">

<head>
    <meta charset="utf-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">

    <title>Project 5</title>

    <link rel="stylesheet" href="style.css">
    <link href="https://fonts.googleapis.com/css2?family=Montserrat:wght@300;400;600&family=Merriweather:wght@300;400;700&display=swap" rel="stylesheet">

    <script defer src="https://use.fontawesome.com/releases/v5.7.2/js/all.js"
        integrity="sha384-0pzryjIRos8mFBWMzSSZApWtPl/5++eIfzYmTgBBmXYdhvxPc+XcFEk+zJwDgWbP"
        crossorigin="anonymous"></script>
    <script id="MathJax-script" async src="https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-mml-chtml.js"></script>
</head>

<body>
    <header>
        <h1>Project 5A: The Power of Diffusion Models</h1>
        <p>Kishan Jani</p>
    </header>

    <div class="container">

        <!-- Contents -->
        <main>
            </section>
            <section id="part 0">
                <h2>Part 5A 0: Setup</h2>
                <p>
                    We will use the <code> DeepFloydIF </code> diffusion model. We use a random seed of <code> seed = 10</code>, 
                    here and throughout the project. In this part, we simply test out the model, generating images 
                    for 3 text prompts with captions. We do this across different values of <code> num_inference_steps </code>.
                </p>
                <br>
                To see the images generated, click on the following link: 
                <br>
                <a href="./0/index.html">5A Part 0 Results</a>
            </section>

            <section id="part 1.1">
                <h2>Part 5A 1.1: Implementing the Forward Porcess</h2>
                <h3> Overview </h3>
                <p>
                    The forward process is defined by 
                    \[ x_t = \sqrt{\bar{\alpha}_t}x_0 + \sqrt{1 - \bar{\alpha}_t}\varepsilon  \]
                    where \( \varepsilon \sim N(0,1) \). Here \(x_0\) is the clean image, the noisy image 
                    generated is \( x_t \), so we really are making the noisy image by sampling from a Gaussian 
                    of mean \( \sqrt{\bar{\alpha}_t} x_0\) and variance \( 1- \bar{\alpha}_t \). 

                    We perform the process on a test image of the campanile shown below, which we will resize 
                    to \( 64 \times 64 \). 
                </p>
                <h3> Results </h3>
                <p> To see results for \( t\in [250,500,750] \), click the link below: </p>

                <a href="./1.1/index.html">5A Part 1.1 Results</a>
                  
            </section>


            <section id="part 1.2">
                <h2>Part 5A 1.2: Gaussian Blur Denoising</h2>
                <h3> Overview </h3>
                <p>
                    We naively denoise images by blurring them with a fixed Gaussian. We use
                    <code>kernel_size = 5</code> and <code> sigma = 1.5 </code> for the Gaussian. We do
                    this for the noise levels seen before. 
                </p>
                <h3> Results </h3>
                <p> To see results for \( t\in [250,500,750] \), click the link below: </p>

                <a href="./1.2/index.html">5A Part 1.2 Results</a>
                  
            </section>

            <section id="part 1.3">
                <h2>Part 5A 1.3: One-Step Denoising</h2>
                <h3> Overview </h3>
                <p>
                    Now, we'll use a pretrained diffusion model to denoise. The actual denoiser
                     can be found at <code>stage_1.unet</code>. This is a UNet that has already been trained on 
                     a very, very large dataset of pairs of images \( x_0, x_t \). We can use it to 
                     recover Gaussian noise from the image. Then, we can remove this noise to recover 
                     (something close to) the original image. To compute \(x_0\) from \(x_t\), we use 
                     \[ \hat{x}_0 = \frac{1}{\sqrt{\bar{\alpha}_t}} [x_t - \sqrt{1 - \bar{\alpha}_t} \varepsilon_\theta(x_t,t)],\]
                     where \(\varepsilon_\theta(\cdot,\cdot)\) is the noise predicted by our model for a given noisy image \(x_t\)
                     and corresponding noise level \(t\).  
                </p>
                <h3> Results </h3>
                <p> To see results for \( t\in [250,500,750] \), click the link below: </p>

                <a href="./1.3/index.html">5A Part 1.3 Results</a>
                  
            </section>

            <section id="part 1.4">
                <h2>Part 5A 1.4: Iterative Denoising</h2>
                <h3> Overview </h3>
                <p>
                    We can denoise an image and recover more of the original features by denoising several times 
                    in small steps instead of denoising in one-step. Specifically, over \(T=1000\) iterations (step = -30), 
                    we denoise the image iteratively as follows: 
                    \[ x_{t'} = \frac{\sqrt{\bar{\alpha}_{t'}} \beta_t} {1 - \bar{\alpha}_t} x_0 
                    + \frac{\sqrt{\alpha_t}(1-\bar{\alpha}_{t'})}{1-\bar{\alpha}_t} x_t + v_\sigma,\]
                    where \(x_0\) is our current clean estimate via a one-step estimate from \(x_t\), \(v_\sigma\) is random noise (predicted for DeepFloyd), 
                    alphas and betas are known parameters. Here \(x_t\) is the more noisy image at some timestep \(t\), while 
                    \(x_{t'}\) is the less noisy image at the next timestep \(t'\) in our iteration sequence. 
                    
                </p>
                <h3> Results </h3>
                <p> To see results, click the link below: </p>

                <a href="./1.4/index.html">5A Part 1.4 Results</a>
                  
            </section>

            <section id="part 1.5">
                <h2>Part 5A 1.5: Diffusion Model Sampling</h2>
                <h3> Overview </h3>
                <p>
                    We "generate" images using the Diffusion Model by feeding random noise into the iterative 
                    denoiser, using <code> i_start = 0 </code>. By starting with random noise as the noisy image, 
                    we are "solving" towards a random image via iterative denoising. We use the text prompt "a high 
                    quality photo" for denoising. 
                </p>
                <h3> Results </h3>
                <p> To see results, click the link below: </p>

                <a href="./1.5/index.html">5A Part 1.5 Results</a>
                  
            </section>

            <section id="part 1.6">
                <h2>Part 5A 1.6: Classifier Free Guidance (CFG)</h2>
                <h3> Overview </h3>
                <p>
                    We can generate higher quality images by implementing conditional diffusion with prompt 
                    "a high quality photo" (using unconditional prompt ""). With scale \( \gamma = 7\), this 
                    ensures that our model diffuses the image towards the prompt being fed to generate 
                    the conditional noise. Specifically, we skew our noise estimate as 
                    \[\varepsilon = \varepsilon_{\text{uncond}} + \gamma(\varepsilon_{\text{cond}} - 
                    \varepsilon_{\text{uncond}} ).\]
                    With \(\gamma=0\), we have the standard unconditional noise estimate, while with \(\gamma=1\), 
                    we have the conditional noise estimate. It has been determined empirically that 
                    choosing \(\gamma > 1\) improves conditioning. This generates images of higher quality, albeit 
                    slightly lesser variance. 
                </p>
                <h3> Results </h3>
                <p> To see results, click the link below: </p>

                <a href="./1.6/index.html">5A Part 1.6 Results</a>
                  
            </section>

            <section id="part 1.7">
                <h2>Part 5A 1.7: Image-to-Image translation</h2>
                <h3> Overview </h3>
                <p>
                    We create various edited images using Diffusion methods.
                </p>
                <h3> Results </h3>
                <p> To see results, click the link below: </p>

                <a href="./1.7/index.html">5A Part 1.7 Results</a>
                  
            </section>

            <section id="part 1.8">
                <h2>Part 5A 1.8: Visual Anagrams</h2>
                <h3> Overview </h3>
                <p>
                    Similar to inpainting, we can make other fancy edits to the noise to create interesting images. 
                    Over here, we develop visual anagrams: oriented normally, the image looks like <code>prompt1</code>, 
                    but when flipped, the image looks like <code>prompt2</code>. Once again, within our denoising loop, 
                    we make the following edit to noisy image \( x_t\): 
                    \[\varepsilon_1 \gets \textup{UNet}(x_t,t, \text{prompt}_1) \]
                    \[\varepsilon_2 \gets \textup{UNet}(x_t,t, \text{prompt}_2) \]
                    \[\varepsilon \gets \frac{\varepsilon_1 + \varepsilon_2}{2},\]
                    where \( \varepsilon \) is the final noise estimate for iteration \(t \). 
                </p>
                <h3> Results </h3>
                <p> To see results, click the link below: </p>

                <a href="./1.8/index.html">5A Part 1.8 Results</a>
                  
            </section>

            <section id="part 1.9">
                <h2>Part 5A 1.9: Hybrid Images</h2>
                <h3> Overview </h3>
                <p>
                    We create hybrid images: smaller image / from afar looks like <code>prompt1</code>, but 
                    larger image / from close-by looks like <code>prompt2</code>. We accomplish this by using low
                    and high pass filters, implemented via a Gaussian of kernel 33 and sigma 2. For any iteration
                    of the denoising loop, 
                    \[\varepsilon_1 \gets \textup{UNet}(x_t,t, \text{prompt}_1) \]
                    \[\varepsilon_2 \gets \textup{UNet}(x_t,t, \text{prompt}_2) \]
                    \[\varepsilon \gets f_{\text{low}}(\varepsilon_1) + f_{\text{high}}(\varepsilon_2),\]
                    where \( \varepsilon \) is the final noise estimate for iteration \(t \). 
                </p>
                <h3> Results </h3>
                <p> To see results, click the link below: </p>

                <a href="./1.9/index.html">5A Part 1.9 Results</a>
                  
            </section>
        </main>
    </div>

</body>

</html>
