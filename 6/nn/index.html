<!doctype html>
<html lang="en">

<head>
    <meta charset="utf-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">

    <title>CNN</title>

    <link rel="stylesheet" href="style.css">
    <link href="https://fonts.googleapis.com/css2?family=Montserrat:wght@300;400;600&family=Merriweather:wght@300;400;700&display=swap" rel="stylesheet">

    <script defer src="https://use.fontawesome.com/releases/v5.7.2/js/all.js"
        integrity="sha384-0pzryjIRos8mFBWMzSSZApWtPl/5++eIfzYmTgBBmXYdhvxPc+XcFEk+zJwDgWbP"
        crossorigin="anonymous"></script>
    <script id="MathJax-script" async src="https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-mml-chtml.js"></script>
</head>

<body>
    <header>
        <h1>Final Project 2: Facial Keypoint Detection using Convolutional Neural Nets</h1>
        <p>Kishan Jani</p>
    </header>

    <div class="container">

        <!-- Contents -->
        <main>
            <section id="part 1">
                <h2>Part 1: Nose Tip Detection</h2>
                <h3>Overview</h3>
                <p>
                    In this project, we use the IMM Face Database for training an initial toy model for nose tip detection. 
                    The dataset contains 240 facial images of 40 individuals, with each person having 6 images taken from 
                    various viewpoints. Each image is annotated with 58 facial keypoints. 
                </p>
                <p>
                    For training, we use all 6 images of the first 32 individuals (indexes 1-32), resulting in a total 
                    of 192 training images. The remaining 8 individuals (indexes 33-40) provide 48 images for validation. 
                </p>
                <p>
                    The nose detection task is cast as a pixel coordinate regression problem. Each input is a grayscale 
                    image, and the output is the nose tip position represented as normalized coordinates <code>(x, y)</code>, 
                    ranging from 0 to 1.
                </p>
            

                <h3>Data Preparation</h3>
                <p>
                    To load and preprocess the images, we use the <code>torch.utils.data.DataLoader</code>. Each image is 
                    converted to grayscale, normalized to the range -0.5 to 0.5 using the formula 
                    <code>image.astype(np.float32) / 255 - 0.5</code>, and resized to 80x60 pixels.
                </p>
                <p> Some sample faces from the dataset with nose keypoints labelled </p>

                <div class="gallery">
                    <div class="gallery-item">
                        <img src="nn_images/part1/gt1.png" class="medium">
                        <div class="caption">Sample</div>
                    </div>

                    <div class="gallery-item">
                        <img src="nn_images/part1/gt2.png" class="medium">
                        <div class="caption">Sample</div>
                    </div>

                    <div class="gallery-item">
                        <img src="nn_images/part1/gt3.png" class="medium">
                        <div class="caption">Sample</div>
                    </div>
                </div> 

                <div class="gallery">
                    <div class="gallery-item">
                        <img src="nn_images/part1/gt_samp.png" class="medium">
                        <div class="caption">Sample with all keypoints</div>
                    </div>

                    <div class="gallery-item">
                        <img src="nn_images/part1/gt6.png" class="medium">
                        <div class="caption">Sample</div>
                    </div>

                    <div class="gallery-item">
                        <img src="nn_images/part1/gt5.png" class="medium">
                        <div class="caption">Sample</div>
                    </div>
                </div> 
            
                <h3>Model Architecture</h3>
                <p>
                    The model is a convolutional neural network (CNN) built using <code>torch.nn.Module</code>. The architecture 
                    consists of convolutional layers (<code>torch.nn.Conv2d</code>) followed by ReLU activations and max-pooling 
                    layers (<code>torch.nn.MaxPool2d</code>). After the convolutional layers, the network includes two fully 
                    connected layers, with a ReLU activation after the first but not the last. Below is an image describing the precise 
                    architecture. 

                            <img src="nn_images/part1/nose_model.png">

                            <br>
                            Sequence of operations used was 

                            <pre>
                                <code>[Conv->Relu->Pool]*3 -> [FC1->Relu] -> FC2</code>
                            </pre>

                            I initially experimented with using only 7x7 and 5x5 convolutional kernels. 
                            However, these larger kernels proved challenging to train effectively given the 
                            limited data and the small size of the images. They were too aggressive for this 
                            context, and the learning rate further exacerbated the issue. A higher learning 
                            rate was necessary to achieve sufficient training progress, but this often led to 
                            overfitting.

                            In contrast, using smaller 3x3 kernels throughout the model consistently produced 
                            better results in my experiments. While this outcome intuitively defied my 
                            expectations—particularly considering the success of downsampling/upsampling kernels in architectures 
                            like UNet—it demonstrates that smaller kernels can strike a better balance between 
                            learning efficiency and generalization for this specific task and dataset.         
                </p>
                <p>
                    The loss function is <code>torch.nn.MSELoss</code>, and the optimizer is Adam 
                    (<code>torch.optim.Adam</code>) with a learning rate of <code>1e-3</code>. The network is trained for 10 to 25 epochs.
                    <code>batch_size = 4</code> because the dataset is fairly small, and we want to prevent overfitting. 
                </p>

                <h3>Training/Validation Loss Plot</h3>
                <p>
                    Below is an example of the training and validation loss plot over several epochs. Loss stabilizes, indicating 
                    the need for significantly many more epochs for further improvement. At the end, the training loss was <code>0.000245</code>
                    and validation loss was <code>.001123</code>. 
                </p>
                <img src="nn_images/part1/tr_val_loss.png" alt="Training and Validation Loss Plot">
            
                <h3>Results</h3>
                <p>
                    The model performs well for faces with straightforward orientation and neutral expressions. However, it struggles 
                    with images containing strong lighting contrasts, unusual facial expressions, or tilted faces. In some cases, 
                    other parts of the face (e.g., the cheekbone) are incorrectly identified as the nose tip.
                </p>
                <p>
                    <strong>Good Performance</strong>
                </p>
                <div class="gallery">
                    <div class="gallery-item">
                        <img src="nn_images/part1/good1.png" class="medium">
                        <div class="caption">Straight face and symmetric, well-defined features.</div>
                    </div>

                    <div class="gallery-item">
                        <img src="nn_images/part1/good2.png" class="medium">
                        <div class="caption">Good Prediction for same reasons</div>
                    </div>

                    <div class="gallery-item">
                        <img src="nn_images/part1/good4.png" class="medium">
                        <div class="caption">Identical</div>
                    </div>
                </div>

                <p>
                    <strong>Bad Performance</strong>
                </p>
                <div class="gallery">
                    <div class="gallery-item">
                        <img src="nn_images/part1/bad1.png" class="medium">
                        <div class="caption">Cheekbone interpreted as nose</div>
                    </div>

                    <div class="gallery-item">
                        <img src="nn_images/part1/bad4.png" class="medium">
                        <div class="caption">Facial Expression, Cheekbone Problem (contrast)</div>
                    </div>

                    <div class="gallery-item">
                        <img src="nn_images/part1/bad3.png" class="medium">
                        <div class="caption">Tilted face, Cheekbone Problem (contrast)</div>
                    </div>
                </div>
   
            </section>
            
            <section id="part 2">
                <h2>Part 2: Full Face Detection</h2>
                <h3>Overview</h3>

                <p>
                    In this section, we focus on predicting all keypoints for facial landmarks. The dataset and model 
                    architecture have to be adapted to handle larger input image sizes and include techniques to mitigate 
                    overfitting.
                </p>
                <h3>Data Preparation</h3>

                <p>
                    Similar to Part 1, the dataloader code remains largely unchanged. However, this time we work with larger 
                    input image sizes of <code>160x120</code>. Given the small dataset, data augmentation is essential to 
                    prevent overfitting. We apply the following augmentations:
                </p>
                <ul>
                    <li>Randomly changing brightness and saturation using <code>torchvision.transforms.ColorJitter</code>.</li>
                    <li>Random rotations between -15 and 15 degrees.</li>
                    <li>Random shifts of up to ±10 pixels.</li>
                </ul>

                <div class="gallery">
                    <div class="gallery-item">
                        <img src="nn_images/part2/gt1.png" class="medium">
                        <div class="caption">Sample</div>
                    </div>

                    <div class="gallery-item">
                        <img src="nn_images/part2/gt2.png" class="medium">
                        <div class="caption">Sample</div>
                    </div>

                    <div class="gallery-item">
                        <img src="nn_images/part2/gt4.png" class="medium">
                        <div class="caption">Sample</div>
                    </div>
                </div> 
            
                <h3>Model Architecture</h3>

                <p>
                    With larger input images, a deeper CNN is required. For this task, we design a model with 4 convolutional 
                    layers, each followed by a ReLU activation and optionally a max-pooling layer. Based on observations from the 
                    previous section, we use only 5x5 and 3x3 convolutional kernels.
                </p>
                <p>
                    To reduce overfitting, we introduce dropout layers, ensuring that the model does not over-rely on specific 
                    features. Additionally, a normal bias is applied to the final layer to guide predictions closer to the face, 
                    which significantly improves validation loss. Without this adjustment, the validation loss stabilizes at a 
                    higher magnitude. The model is trained with a batch size of 16, using the Adam optimizer (learning rate 
                    <code>1e-3</code>) and MSE loss. Below is the architecture 
                    <img src="nn_images/part2/all_model.png">
                </p>

                <p>
                    Order of operations chosen: 
                    <pre>
                            <code> [Conv -> Relu] -> [Conv->Relu->Pool]*3 -> DP -> [FC1->Relu] -> [FC2-> Relu] -> [FC3] </code>
                    </pre>
                        <ul>
                            <li>We do not use max pooling in the first layer to better preserve information across layers, helping to prevent overfitting.</li>
                            <li>Dropout accelerates convergence during training.</li>
                            <li>The final fully connected layer does not include a ReLU activation, as suggested.</li>
                        </ul>
                </p>

                <h3>Training/Validation Loss Plot</h3>

                <p>
                    The training process shows promise, with validation loss oscillating and decreasing over time, though the 
                    validation curve is somewhat non-smooth. Extending the number of training epochs could yield further 
                    improvements.
                </p>
            
                <img src="nn_images/part2/tr_val_loss.png" alt="Training and Validation Loss Plot">
                <h3>Results</h3>

                <p>
                    Overall, the model demonstrates strong facial prediction capabilities. However, certain challenges remain, 
                    particularly with outlier features. For example:
                </p>
                <ul>
                    <li>Faces with extreme tilts or contrasting lighting conditions tend to throw off the model, causing it to 
                    misidentify keypoints (e.g., predicting a cheekbone instead of the nose).</li>
                    <li>In these cases, the neural network struggles with rare scenarios that are underrepresented in the dataset.</li>
                </ul>
                <p>
                    A key improvement would be to introduce more significant data augmentations or include additional training 
                    data with diverse facial angles and lighting conditions. Despite these challenges, the model performs well 
                    on many tilted faces, with failures limited to extreme cases.
                </p>

                <p>
                    Below are some examples:
                </p>

                <p>
                    <strong>Good Performance</strong>
                </p>
                <div class="gallery">
                    <div class="gallery-item">
                        <img src="nn_images/part2/good1.png" class="medium">
                        <div class="caption">With Tilt</div>
                    </div>

                    <div class="gallery-item">
                        <img src="nn_images/part2/good2.png" class="medium">
                        <div class="caption">Straightforward prediction</div>
                    </div>

                    <div class="gallery-item">
                        <img src="nn_images/part2/good4.png" class="medium">
                        <div class="caption">Extreme Facial Tilt, still good</div>
                    </div>
                </div>

                <p>
                    <strong>Bad Performance</strong>
                </p>
                <div class="gallery">
                    <div class="gallery-item">
                        <img src="nn_images/part2/bad1.png" class="medium">
                        <div class="caption">Too much contrast</div>
                    </div>

                    <div class="gallery-item">
                        <img src="nn_images/part2/bad4.png" class="medium">
                        <div class="caption">Facial Expression, Cheekbone Problem (contrast)</div>
                    </div>

                    <div class="gallery-item">
                        <img src="nn_images/part2/bad2.png" class="medium">
                        <div class="caption">Facial features induce more contrast</div>
                    </div>
                </div>

            </section>

            <section id="part 3">
                <h2>Part 3: Larger Datasets</h2>
                <h3>Overview</h3>

                <p>
                    In this section, we use the iBUG Face in the Wild dataset to train a robust facial keypoints detector. 
                    This dataset consists of 6666 images of varying sizes, each annotated with 68 facial keypoints. These 
                    keypoints represent critical facial landmarks such as the eyes, nose, mouth, and jawline, making it an 
                    excellent resource for facial analysis tasks.
                </p>
                <h3>Data Preparation</h3>

                <p>
                    In the iBUG dataset, faces often occupy only a small portion of the image, with the background dominating 
                    most of the space. To address this, we crop each image to focus on the face using the provided bounding 
                    boxes, then resize the cropped face to <code>224x224</code>. After cropping, the coordinates of the keypoints 
                    are updated accordingly to maintain alignment with the resized face.
                </p>
                <p>
                    To improve model robustness and prevent overfitting, we apply the same data augmentation techniques as in 
                    Part 2. Some examples (these also have predictions, but you get the point). 

                    <div class="gallery">
                        <div class="gallery-item">
                            <img src="nn_images/part3/1000.png" class="medium">
                            <div class="caption">Sample</div>
                        </div>
    
                        <div class="gallery-item">
                            <img src="nn_images/part3/20.png" class="medium">
                            <div class="caption">Sample</div>
                        </div>
    
                        <div class="gallery-item">
                            <img src="nn_images/part3/434.png" class="medium">
                            <div class="caption">Sample</div>
                        </div>
                    </div>
            
                <h3>Model Architecture</h3>

                <p>
                    For this part, we use the standard ResNet architecture. Specifically, ResNet18 is a 
                    good choice due to its simplicity and efficiency. Two key modifications are required for this task:
                </p>
                <ul>
                    <li>The first convolutional layer is adjusted to accept a single grayscale channel instead of three RGB channels.</li>
                    <li>The final fully connected layer outputs <code>68 * 2 = 136</code> values, corresponding to the <code>(x, y)</code> 
                    coordinates of the 68 facial landmarks.</li>
                </ul>
                <p>
                    Training is performed with the following hyperparameters:
                </p>
                <ul>
                    <li>Learning rate: <code>5e-3</code>, with a weight decay of <code>1e-5</code> for faster convergence.</li>
                    <li>Batch size: <code>64</code>, to efficiently utilize the larger dataset.</li>
                    <li>Epochs: <code>10</code>, though more training epochs could further improve results.</li>
                </ul>

                <h3>Training/Validation Loss Plot</h3>

                <p>
                    Training and validation loss trends indicate steady progress, though training more epochs could yield better 
                    results. Below is a plot of training and validation loss over iterations:
                </p>

            
                <img src="nn_images/part3/plot.png" >
                <h3>Results</h3>
                <p>
                Here are the learned features for the model: 
                <img src="nn_images/part2/learned.png" >
            </p>
                        <p>
            The trained model performs exceptionally well, with nearly perfect keypoint predictions on most test images. 
            However, there are some notable failure cases:
        </p>
        <ul>
            <li>Faces in low-brightness conditions often result in less accurate predictions. Also, the model is bad at 
                predicting keypoints for darker skin-tones. More subtle and smart data augmentations (or a more diverse dataset)
                is likely the answer. </li>
            <li>Blurry images and non-human faces, such as statues, also present challenges.</li>
        </ul>
        <p>
            Despite these limitations, the model demonstrates strong generalization and consistently accurate predictions. 
            Below are a few test results:
        </p>

                <p>
                    <strong>Good Performance</strong>
                </p>
                <div class="gallery">
                    <div class="gallery-item">
                        <img src="nn_images/part3/correct1.png" class="medium">
                        <div class="caption">Standard </div>
                    </div>

                    <div class="gallery-item">
                        <img src="nn_images/part3/correct2.png" class="medium">
                        <div class="caption">Brightness High</div>
                    </div>

                    <div class="gallery-item">
                        <img src="nn_images/part3/correct3.png" class="medium">
                        <div class="caption">Extremeley Accurate</div>
                    </div>
                </div>

                <div class="gallery">
                    <div class="gallery-item">
                        <img src="nn_images/part3/correct4.png" class="medium">
                        <div class="caption">Straight face, extremeley accurate</div>
                    </div>

                    <div class="gallery-item">
                        <img src="nn_images/part3/correct5.png" class="medium">
                        <div class="caption">Front-facing, accurate.</div>
                    </div>
                </div>

                <p>
                    <strong>Bad Performance</strong>
                </p>
                <div class="gallery">
                    <div class="gallery-item">
                        <img src="nn_images/part3/wrong1.png" class="medium">
                        <div class="caption">Likely due to low pixel intensities.</div>
                    </div>

                    <div class="gallery-item">
                        <img src="nn_images/part3/wrong2.png" class="medium">
                        <div class="caption">Blurry</div>
                    </div>

                    <div class="gallery-item">
                        <img src="nn_images/part3/wrong3.png" class="medium">
                        <div class="caption">Pretty Accurate, high brightness</div>
                    </div>
                </div>

                <div class="gallery">
                    <div class="gallery-item">
                        <img src="nn_images/part3/wrong4.png" class="medium">
                        <div class="caption">low pixel intensity</div>
                    </div>

                    <div class="gallery-item">
                        <img src="nn_images/part3/wrong5.png" class="medium">
                        <div class="caption">Not a human</div>
                    </div>
                </div>

                <h3> Testing Set </h3>
                Now for a few images that the model was not trained on: 
                <div class="gallery">
                    <div class="gallery-item">
                        <img src="nn_images/part3/test1.png" class="medium">
                        <div class="caption">1</div>
                    </div>

                    <div class="gallery-item">
                        <img src="nn_images/part3/test2.png" class="medium">
                        <div class="caption">2</div>
                    </div>

                    <div class="gallery-item">
                        <img src="nn_images/part3/test3.png" class="medium">
                        <div class="caption">3</div>
                    </div>
                </div>

                <div class="gallery">
                    <div class="gallery-item">
                        <img src="nn_images/part3/test4.png" class="medium">
                        <div class="caption">4</div>
                    </div>

                    <div class="gallery-item">
                        <img src="nn_images/part3/test5.png" class="medium">
                        <div class="caption">5</div>
                    </div>
                </div>         
            </section>

            <section id="part 4">
                <h2> Part 4: Pixelwise Classification</h2>
            </section>

            Too much space was needed, see link: 
            <br>
            <a href="./pixel/index.html"> Pixelwise Classification Model Results.</a>


            <section id="bw">
                <h2>Bells and Whistles: Zero-One Heatmaps instead of Smooth Gaussians</h2>
                <h3>Overview</h3>
                <p>
                    In this part, we explore the use of zero-one heatmaps for pixelwise classification. The idea is to 
                    replace traditional Gaussian heatmaps with rough, binary zero-one maps, where each keypoint is represented 
                    as a single concentrated pixel. The intuition behind this approach is that the sharply focused heatmap may 
                    offer insights into the Neural Network's learning process by emphasizing precise pixel-level correspondence.
                </p>
            

                <h3>Data Preparation</h3>
                <p>
                    The dataset setup remains the same as in previous sections. Faces are cropped and resized to <code>224x224</code>, 
                    and data augmentation techniques such as brightness and saturation adjustments, rotations, and shifts are 
                    applied to improve model robustness. However, here are the 0-1 heatmaps visualized: 

                <div class="gallery">
                    <div class="gallery-item">
                        <img src="nn_images/partbw/data1.png" class="wide">
                        <div class="caption">Sample</div>
                    </div>

                    <div class="gallery-item">
                        <img src="nn_images/partbw/data2.png" class="wide">
                        <div class="caption">Sample</div>
                    </div>
                </div> 
            
                <h3>Model Architecture</h3>
                <p>
                    The model architecture is identical to that used in Part 4, making it easier to compare the performance of 
                    zero-one heatmaps with traditional Gaussian heatmaps. Logistic loss is employed to optimize the pixelwise 
                    classification task.
                </p>

                <h3>Training/Validation Loss Plot</h3>
                <p>
                    Below is an example of the training and validation loss plot over several epochs. Loss stabilizes, indicating 
                    the need for significantly many more epochs for further improvement. At the end, the training loss was <code>0.000245</code>
                    and validation loss was <code>.001123</code>. 
                </p>
                <img src="nn_images/partbw/zero_one_plot.png" alt="Training and Validation Loss Plot">
            
                <h3>Results</h3>
                <p>
                    Using zero-one heatmaps offers a more defined and direct correspondence between keypoints and heatmaps. This 
                    clarity aids the training process to some extent, as the network is forced to focus on precise pixel-level 
                    predictions. However, the results below highlight several limitations of this approach:
                </p>
                <ul>
                    <li>
                        <strong>Limited Spatial Context:</strong> Zero-one heatmaps provide little spatial context, as they lack 
                        the spread-out representation of Gaussian heatmaps. This absence of broader image information limits 
                        the network's ability to make robust predictions.
                    </li>
                    <li>
                        <strong>Concentrated Predictions:</strong> The keypoints predicted by the network tend to cluster too 
                        closely together. This likely occurs because the network lacks sufficient spatial layers to distribute 
                        the impulse over a wider region, making it difficult to predict keypoints in distant parts of the image.
                        Furthermore, the predictions always make a square shape, which indicates that the filter is not learning new information. Subsequently, 
                        at the very least for outlier/bad predictions, the points are being distributed randomly and uniformly (in concentrated spots near keypoints)
                    </li>
                </ul>
                <p>
                    While zero-one heatmaps offer an interesting alternative, their simplicity may not fully leverage the power 
                    of deep learning for keypoint detection. Adding more spatial layers or hybridizing zero-one heatmaps with 
                    Gaussian-like distributions could be the way to go.
                </p>
                <p>
                    <strong>Good Performance</strong>
                </p>
                <div class="gallery">
                    <div class="gallery-item">
                        <img src="nn_images/partbw/correct5.png" class="medium">
                        <div class="caption">Some degree of facial outline</div>
                    </div>

                    <div class="gallery-item">
                        <img src="nn_images/partbw/correct4.png" class="medium">
                        <div class="caption">Okay</div>
                    </div>

                    <div class="gallery-item">
                        <img src="nn_images/partbw/correct1.png" class="medium">
                        <div class="caption">Decent</div>
                    </div>
                </div>

                <p>
                    <strong>Bad Performance</strong>
                </p>
                <div class="gallery">
                    <div class="gallery-item">
                        <img src="nn_images/partbw/wrong2.png" class="medium">
                        <div class="caption">Blurry, model resorts to random distribution</div>
                    </div>

                    <div class="gallery-item">
                        <img src="nn_images/partbw/wrong4.png" class="medium">
                        <div class="caption">Brightness, same issues outlined above</div>
                    </div>

                    <div class="gallery-item">
                        <img src="nn_images/part1/wrong3.png" class="medium">
                        <div class="caption">A statue (not sharp enough facial features)</div>
                    </div>
                </div>
   
            </section>

            
        </main>
    </div>

</body>

</html>
